---
description: 
globs: 
alwaysApply: true
---
# methodology_logging.mdc

## Trigger
Any time an analytics artifact is generated, modified, or executed including SQL code run against the warehouse, a saved resultset, a dashboard or report artifact in any form, but often HTML.

## Behavior
Cursor must maintain an active log of all analytics workflows in the form of various files. These must be created and maintained during the workflow execution, and not after.  
Whenever Cursor generates dashboards, modifies queries, or helps execute SQL queries, it must maintain a log of those queries.  
For all dashboard migration or evolution type of work, files must be organized alongside the dashboard artifacts. Create a new directory cursor/ wherever you are working to organize your output and log files.  
For all analytics workflows, cursor must create AND MAINTAIN a methodology.md file in the same cursor/ directory as other workflow outputs.  

### 1. Create Timestamped SQL Log
- Append generated SQL to a local `.sql` file with timestamp
- File naming pattern: `YYYY-MM-DD_<workflow title>.sql`
- Generated sql should be separated by a semicolon ;
- SQL must follow Snowflake SQL syntax
- Append all SQL for SIMILAR WORKFLOWS into the same file FOR THAT DATE
- If the analyst begins a new conversation or changes topics, create a new file

#### Include Comprehensive Context
Each SQL entry to the log must include a preceding comment block. DO NOT SUBMIT QUERIES TO THE WAREHOUSE WITH ANY RUN SQL TOOL WITH THE COMMENT BLOCK APPENDED OR IT MAY CREATE SYNTAX ERRORS. ONLY TRY TO EXECUTE CLEAN SQL. ONLY ADD THE COMMENT BLOCK TO THE SQL LOG FILE.

##### Query Metadata
```sql
-- =============================================================================
-- QUERY LOG ENTRY
-- Timestamp: [ISO 8601 format]
-- Session ID: [unique session identifier]
-- User: [if known/available]
-- =============================================================================
```

##### Query Classification
Add classification tags:
```sql
-- TAGS: [exploration, production, debugging, migration, optimization]
-- PRIORITY: [LOW, MEDIUM, HIGH, CRITICAL]
-- REUSABILITY: [one-time, template, recurring]
```


### 2. Create Methodology Documentation File
The methodology.md file must be updated as the process evolves and include:

### Required Sections:

**Analysis Objective**
- Clear statement of the analytical question(s) being answered
- Business context and stakeholder requirements
- Expected deliverables and success criteria

**Data Sources**
- Comprehensive list of all data sources used
- For SQL queries: reference the SQL log file and explain methodology
- For external data: cite specific sources with access dates
- Detail any data quality assumptions or limitations

**Methodology**
- Step-by-step explanation of analytical approach
- WHERE clause justifications and filtering logic
- Aggregation methods and rationale
- Any transformations or calculations performed
- Assumptions made during analysis

**Technical Implementation**
- Tools and technologies used
- Query execution strategy
- Performance considerations
- Error handling approaches

**Validation & Quality Assurance**
- Data validation steps taken
- Cross-checks performed
- Known limitations or caveats
- Confidence levels in results

**Audit Trail**
- Key decisions made during analysis
- Alternative approaches considered
- Reasoning for chosen methodology
- Change log for methodology updates

#### Documentation Standards:
- Use clear, concise language suitable for auditors
- Include specific examples where helpful
- Reference timestamped SQL log entries
- Maintain version history of methodology changes
- Organize sections logically for easy navigation
- Ensure transparency in all analytical decisions

#### Example Output Format
```sql
-- =============================================================================
-- QUERY LOG ENTRY
-- Timestamp: 2024-01-15T14:30:00Z
-- Session ID: sess_abc123
-- User: analyst@company.com
-- =============================================================================
-- BUSINESS CONTEXT:
-- Question/Request: Show daily DEX trading volume on Ethereum for last 30 days
-- Dashboard/Component: DEX Analytics Overview - Volume Trends Chart
-- Stakeholder: Trading Desk Team
-- Expected Output: Line chart visualization
-- =============================================================================
-- TECHNICAL CONTEXT:
-- Query Type: SELECT
-- Data Sources: ethereum.dex.trades, ethereum.core.blocks
-- Complexity: MODERATE
-- Dependencies: Block timestamp calculations
-- Performance Notes: Uses indexed trade_date column
-- =============================================================================
-- BLOCKCHAIN CONTEXT:
-- Protocol(s): Ethereum
-- Metric Type: Volume
-- Time Range: Last 30 days
-- Aggregation: Daily
-- Chain-specific Logic: DEX aggregation across multiple protocols
-- =============================================================================
-- TAGS: production, recurring
-- PRIORITY: HIGH
-- REUSABILITY: template
-- =============================================================================

SELECT 
    DATE_TRUNC('day', block_timestamp) as trade_date,
    SUM(amount_usd) as daily_volume_usd,
    COUNT(*) as total_trades
FROM ethereum.dex.trades t
JOIN ethereum.core.blocks b ON t.block_number = b.block_number
WHERE block_timestamp >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY DATE_TRUNC('day', block_timestamp)
ORDER BY trade_date;

-- =============================================================================
-- END QUERY LOG ENTRY
-- =============================================================================
```

#### Business Context Example
```sql
-- BUSINESS CONTEXT:
-- Question/Request: [what analytical question triggered this query]
-- Dashboard/Component: [which dashboard or component this relates to]
-- Stakeholder: [who initiated the request, if known]
-- Expected Output: [what format/visualization this feeds into]
```

#### Technical Context Example
```sql
-- TECHNICAL CONTEXT:
-- Query Type: [SELECT, INSERT, UPDATE, DELETE, CREATE, etc.]
-- Data Sources: [tables and schemas accessed]
-- Complexity: [SIMPLE, MODERATE, COMPLEX]
-- Dependencies: [other queries or data this depends on]
-- Performance Notes: [expected execution time, optimization notes]
```

#### Blockchain-Specific Context Example
```sql
-- BLOCKCHAIN CONTEXT:
-- Protocol(s): [Ethereum, Polygon, BSC, etc.]
-- Metric Type: [TVL, Volume, Users, Transactions, etc.]
-- Time Range: [specific date range being analyzed]
-- Aggregation: [daily, weekly, monthly, etc.]
-- Chain-specific Logic: [any protocol-specific calculations]
```

### Error Handling and Debugging
When SQL fails or needs debugging:
```sql
-- ERROR LOG:
-- Error Message: [full error text]
-- Attempted Fix: [what was tried to resolve]
-- Resolution: [final working solution]
-- Lessons Learned: [insights for future queries]
```

### Performance Tracking
For complex queries, log:
```sql
-- PERFORMANCE METRICS:
-- Estimated Rows: [expected result set size]
-- Execution Time: [if measured]
-- Resource Usage: [memory, CPU notes]
-- Optimization Applied: [indexes, query hints, etc.]
```

## File Organization
- Create separate directories for different projects/dashboards
- Organize sql log and methodology.md file in the working directory for the task at hand
- Use consistent naming conventions

## Security and Compliance
- Never log sensitive data or credentials
- Redact any PII or confidential information
- Include compliance notes for regulatory requirements

